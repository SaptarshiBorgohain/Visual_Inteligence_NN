# GENERATED BY CODING AGENT: Implement Visual Physics World-Model per spec.
# Target device: Mac M-series (MPS) if available. Keep float32; test shapes & parameter counts.
# Provide clear docstrings.

"""
utils.py - Utility functions for device selection, save/load helpers, and parameter counting.
"""

import torch
import numpy as np
import random
import os
from typing import Optional, Dict, Any


def get_device() -> torch.device:
    """
    Select the best available device.
    Priority: MPS (Apple Silicon) > CUDA > CPU
    
    Returns:
        torch.device: The selected device.
    """
    if torch.backends.mps.is_available():
        return torch.device("mps")
    elif torch.cuda.is_available():
        return torch.device("cuda")
    else:
        return torch.device("cpu")


def seed_everything(seed: int = 42) -> None:
    """
    Set random seeds for reproducibility.
    
    Args:
        seed: Random seed value.
    """
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    # For deterministic behavior (may impact performance)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def count_parameters(model: torch.nn.Module, trainable_only: bool = True) -> int:
    """
    Count the number of parameters in a model.
    
    Args:
        model: PyTorch model.
        trainable_only: If True, count only trainable parameters.
        
    Returns:
        int: Number of parameters.
    """
    if trainable_only:
        return sum(p.numel() for p in model.parameters() if p.requires_grad)
    else:
        return sum(p.numel() for p in model.parameters())


def print_param_table(model: torch.nn.Module) -> None:
    """
    Print a detailed table of parameter counts per module.
    
    Args:
        model: PyTorch model with encoder, dynamics, decoder attributes.
    """
    print("\n" + "=" * 60)
    print("PARAMETER COUNT TABLE")
    print("=" * 60)
    
    total = count_parameters(model)
    
    if hasattr(model, 'encoder'):
        enc_params = count_parameters(model.encoder)
        print(f"  Encoder:    {enc_params:>12,} params ({enc_params/1e6:.3f}M)")
    
    if hasattr(model, 'dynamics'):
        dyn_params = count_parameters(model.dynamics)
        print(f"  Dynamics:   {dyn_params:>12,} params ({dyn_params/1e6:.3f}M)")
    
    if hasattr(model, 'decoder'):
        dec_params = count_parameters(model.decoder)
        print(f"  Decoder:    {dec_params:>12,} params ({dec_params/1e6:.3f}M)")
    
    print("-" * 60)
    print(f"  TOTAL:      {total:>12,} params ({total/1e6:.3f}M)")
    print("=" * 60)
    
    # Reference budgets (actual with full GRU architecture)
    print("\nReference parameter counts (with GRU):")
    print("  small  ≈ 5M")
    print("  medium ≈ 10M")
    print("  large  ≈ 20M")
    print()


def save_checkpoint(
    model: torch.nn.Module,
    optimizer: torch.optim.Optimizer,
    epoch: int,
    config: Dict[str, Any],
    path: str,
    scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
    metrics: Optional[Dict[str, float]] = None
) -> None:
    """
    Save a training checkpoint.
    
    Args:
        model: The model to save.
        optimizer: The optimizer state.
        epoch: Current epoch number.
        config: Training configuration dictionary.
        path: Path to save the checkpoint.
        scheduler: Optional learning rate scheduler.
        metrics: Optional metrics dictionary.
    """
    checkpoint = {
        "model_state": model.state_dict(),
        "optim_state": optimizer.state_dict(),
        "epoch": epoch,
        "config": config,
    }
    
    if scheduler is not None:
        checkpoint["scheduler_state"] = scheduler.state_dict()
    
    if metrics is not None:
        checkpoint["metrics"] = metrics
    
    # Ensure directory exists
    os.makedirs(os.path.dirname(path) if os.path.dirname(path) else ".", exist_ok=True)
    
    torch.save(checkpoint, path)
    print(f"Checkpoint saved: {path}")


def load_checkpoint(
    path: str,
    model: torch.nn.Module,
    optimizer: Optional[torch.optim.Optimizer] = None,
    scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
    device: Optional[torch.device] = None
) -> Dict[str, Any]:
    """
    Load a training checkpoint.
    
    Args:
        path: Path to the checkpoint file.
        model: Model to load weights into.
        optimizer: Optional optimizer to load state into.
        scheduler: Optional scheduler to load state into.
        device: Device to map the checkpoint to.
        
    Returns:
        Dict containing epoch, config, and any stored metrics.
    """
    if device is None:
        device = get_device()
    
    checkpoint = torch.load(path, map_location=device)
    
    model.load_state_dict(checkpoint["model_state"])
    print(f"Model weights loaded from {path}")
    
    if optimizer is not None and "optim_state" in checkpoint:
        optimizer.load_state_dict(checkpoint["optim_state"])
        print("Optimizer state loaded")
    
    if scheduler is not None and "scheduler_state" in checkpoint:
        scheduler.load_state_dict(checkpoint["scheduler_state"])
        print("Scheduler state loaded")
    
    return {
        "epoch": checkpoint.get("epoch", 0),
        "config": checkpoint.get("config", {}),
        "metrics": checkpoint.get("metrics", {})
    }


def get_config(profile: str = "large") -> Dict[str, Any]:
    """
    Get configuration for a specific profile.
    
    Args:
        profile: One of "small", "medium", "large".
        
    Returns:
        Configuration dictionary.
    """
    configs = {
        "small": {
            "profile": "small",
            "encoder_channels": [48, 96, 192],
            "latent_dim": 128,
            "dynamics_hidden": 512,
            "decoder_channels": [48, 96, 192],
            "in_channels": 3,
            "image_size": 64,
            "batch_size": 32,
            "lr": 3e-4,
            "weight_decay": 1e-5,
            "epochs": 100,
            "alpha_pixel": 1.0,
            "beta_latent": 0.5,
            "gamma_smooth": 1e-3,
            "use_gru": True,
            "use_amp": False,
        },
        "medium": {
            "profile": "medium",
            "encoder_channels": [64, 128, 256],
            "latent_dim": 192,
            "dynamics_hidden": 768,
            "decoder_channels": [64, 128, 256],
            "in_channels": 3,
            "image_size": 64,
            "batch_size": 16,
            "lr": 3e-4,
            "weight_decay": 1e-5,
            "epochs": 100,
            "alpha_pixel": 1.0,
            "beta_latent": 0.5,
            "gamma_smooth": 1e-3,
            "use_gru": True,
            "use_amp": False,
        },
        "large": {
            "profile": "large",
            "encoder_channels": [128, 256, 512],
            "latent_dim": 256,
            "dynamics_hidden": 1024,
            "decoder_channels": [128, 256, 512],
            "in_channels": 3,
            "image_size": 64,
            "batch_size": 8,
            "lr": 3e-4,
            "weight_decay": 1e-5,
            "epochs": 100,
            "alpha_pixel": 1.0,
            "beta_latent": 0.5,
            "gamma_smooth": 1e-3,
            "use_gru": True,
            "use_amp": False,
        }
    }
    
    if profile not in configs:
        raise ValueError(f"Unknown profile: {profile}. Choose from: {list(configs.keys())}")
    
    return configs[profile]


def format_time(seconds: float) -> str:
    """Format seconds into a human-readable string."""
    if seconds < 60:
        return f"{seconds:.1f}s"
    elif seconds < 3600:
        return f"{seconds // 60:.0f}m {seconds % 60:.0f}s"
    else:
        hours = seconds // 3600
        minutes = (seconds % 3600) // 60
        return f"{hours:.0f}h {minutes:.0f}m"


def compute_psnr(mse: float, max_val: float = 1.0) -> float:
    """
    Compute Peak Signal-to-Noise Ratio.
    
    Args:
        mse: Mean Squared Error.
        max_val: Maximum pixel value (1.0 for normalized images).
        
    Returns:
        PSNR in dB.
    """
    if mse == 0:
        return float('inf')
    return 10 * np.log10((max_val ** 2) / mse)
