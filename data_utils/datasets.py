# GENERATED BY CODING AGENT: Implement Visual Physics World-Model per spec.
# Target device: Mac M-series (MPS) if available. Keep float32; test shapes & parameter counts.
# Provide clear docstrings.

"""
datasets.py - Dataset loaders for Visual Physics World-Model training.

Provides wrappers for:
    - MovingMNIST: Classic benchmark for video prediction (20 frames, 64x64).
    - PhysicalConcepts: Placeholder for physics-based datasets (15 frames, 64x64).
    - SyntheticPhysics: A synthetic dataset for testing with simple motion.
"""

import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
from typing import Tuple, Optional, Callable


class MovingMNISTDataset(Dataset):
    """
    Moving MNIST dataset wrapper.
    
    Expected data format: (N, T, H, W) where frames are grayscale.
    This wrapper converts to (N, T, C, H, W) with C=1 or C=3 (replicated).
    
    Args:
        data_path: Path to the .npy file containing MovingMNIST data.
        seq_length: Number of frames per sequence (default 20).
        image_size: Target image size (will resize if different).
        to_rgb: If True, replicate grayscale to 3 channels.
        transform: Optional transform to apply to frames.
        normalize: If True, normalize to [0, 1].
    """
    
    def __init__(
        self,
        data_path: str,
        seq_length: int = 20,
        image_size: int = 64,
        to_rgb: bool = True,
        transform: Optional[Callable] = None,
        normalize: bool = True
    ):
        super().__init__()
        
        self.seq_length = seq_length
        self.image_size = image_size
        self.to_rgb = to_rgb
        self.transform = transform
        self.normalize = normalize
        
        # Load data
        # Expected shape: (N, T, H, W) or (T, N, H, W)
        self.data = np.load(data_path)
        
        # Handle different formats
        if self.data.ndim == 4:
            # Assume (N, T, H, W) or (T, N, H, W)
            if self.data.shape[0] == seq_length:
                # (T, N, H, W) -> (N, T, H, W)
                self.data = np.transpose(self.data, (1, 0, 2, 3))
        
        self.n_samples = self.data.shape[0]
    
    def __len__(self) -> int:
        return self.n_samples
    
    def __getitem__(self, idx: int) -> torch.Tensor:
        """
        Get a sequence of frames.
        
        Returns:
            frames: Tensor of shape (T, C, H, W) normalized to [0, 1].
        """
        # Get sequence: (T, H, W)
        frames = self.data[idx, :self.seq_length].astype(np.float32)
        
        # Normalize to [0, 1]
        if self.normalize and frames.max() > 1.0:
            frames = frames / 255.0
        
        # Convert to tensor: (T, H, W)
        frames = torch.from_numpy(frames)
        
        # Add channel dimension: (T, 1, H, W)
        frames = frames.unsqueeze(1)
        
        # Resize if needed
        if frames.shape[-1] != self.image_size or frames.shape[-2] != self.image_size:
            frames = torch.nn.functional.interpolate(
                frames, size=(self.image_size, self.image_size), mode='bilinear', align_corners=False
            )
        
        # Convert to RGB if requested: (T, 3, H, W)
        if self.to_rgb:
            frames = frames.repeat(1, 3, 1, 1)
        
        # Apply transform
        if self.transform is not None:
            frames = self.transform(frames)
        
        return frames


class PhysicalConceptsDataset(Dataset):
    """
    Placeholder dataset for physics-based video data.
    
    Expected to load data from a directory or file with format:
    - Sequences of 15 frames at 64x64 resolution
    - RGB images normalized to [0, 1]
    
    Args:
        data_path: Path to the dataset directory or file.
        seq_length: Number of frames per sequence (default 15).
        image_size: Target image size.
        transform: Optional transform to apply.
    """
    
    def __init__(
        self,
        data_path: str,
        seq_length: int = 15,
        image_size: int = 64,
        transform: Optional[Callable] = None
    ):
        super().__init__()
        
        self.data_path = data_path
        self.seq_length = seq_length
        self.image_size = image_size
        self.transform = transform
        
        # Attempt to load data
        # This is a placeholder - actual implementation depends on data format
        try:
            self.data = np.load(data_path)
            if self.data.ndim == 5:
                # (N, T, C, H, W)
                self.n_samples = self.data.shape[0]
            elif self.data.ndim == 4:
                # (N, T, H, W) grayscale
                self.n_samples = self.data.shape[0]
            else:
                raise ValueError(f"Unexpected data shape: {self.data.shape}")
        except Exception as e:
            print(f"Warning: Could not load data from {data_path}: {e}")
            print("Using synthetic data instead.")
            self.data = None
            self.n_samples = 1000  # Synthetic fallback
    
    def __len__(self) -> int:
        return self.n_samples
    
    def __getitem__(self, idx: int) -> torch.Tensor:
        """
        Get a sequence of frames.
        
        Returns:
            frames: Tensor of shape (T, C, H, W) normalized to [0, 1].
        """
        if self.data is not None:
            frames = self.data[idx, :self.seq_length]
            
            # Handle grayscale
            if frames.ndim == 3:
                frames = np.expand_dims(frames, axis=1)
                frames = np.repeat(frames, 3, axis=1)
            
            frames = frames.astype(np.float32)
            if frames.max() > 1.0:
                frames = frames / 255.0
            
            frames = torch.from_numpy(frames)
        else:
            # Generate synthetic data
            frames = self._generate_synthetic(idx)
        
        if self.transform is not None:
            frames = self.transform(frames)
        
        return frames
    
    def _generate_synthetic(self, idx: int) -> torch.Tensor:
        """Generate synthetic physics-like motion."""
        np.random.seed(idx)
        
        frames = torch.zeros(self.seq_length, 3, self.image_size, self.image_size)
        
        # Simple ball motion with physics
        ball_radius = 5
        x = np.random.uniform(10, self.image_size - 10)
        y = np.random.uniform(10, self.image_size - 10)
        vx = np.random.uniform(-3, 3)
        vy = np.random.uniform(-3, 3)
        
        color = np.random.rand(3)
        
        for t in range(self.seq_length):
            # Update position
            x += vx
            y += vy
            
            # Bounce off walls
            if x <= ball_radius or x >= self.image_size - ball_radius:
                vx = -vx
                x = np.clip(x, ball_radius, self.image_size - ball_radius)
            if y <= ball_radius or y >= self.image_size - ball_radius:
                vy = -vy
                y = np.clip(y, ball_radius, self.image_size - ball_radius)
            
            # Draw ball
            for i in range(self.image_size):
                for j in range(self.image_size):
                    if (i - y) ** 2 + (j - x) ** 2 <= ball_radius ** 2:
                        frames[t, :, i, j] = torch.tensor(color)
        
        return frames


class SyntheticPhysicsDataset(Dataset):
    """
    Synthetic dataset with simple physics-based motion for testing.
    
    Generates sequences of balls with constant velocity bouncing off walls.
    Useful for development and testing when real data is not available.
    
    Args:
        n_samples: Number of sequences to generate.
        seq_length: Number of frames per sequence.
        image_size: Size of each frame (image_size x image_size).
        n_balls: Number of balls per sequence.
        in_channels: Number of color channels (1 or 3).
    """
    
    def __init__(
        self,
        n_samples: int = 1000,
        seq_length: int = 20,
        image_size: int = 64,
        n_balls: int = 2,
        in_channels: int = 3
    ):
        super().__init__()
        
        self.n_samples = n_samples
        self.seq_length = seq_length
        self.image_size = image_size
        self.n_balls = n_balls
        self.in_channels = in_channels
    
    def __len__(self) -> int:
        return self.n_samples
    
    def __getitem__(self, idx: int) -> torch.Tensor:
        """
        Generate a sequence with bouncing balls.
        
        Returns:
            frames: Tensor of shape (T, C, H, W) in [0, 1].
        """
        np.random.seed(idx)
        
        frames = torch.zeros(
            self.seq_length, self.in_channels, self.image_size, self.image_size
        )
        
        # Initialize balls
        balls = []
        for _ in range(self.n_balls):
            ball = {
                'x': np.random.uniform(10, self.image_size - 10),
                'y': np.random.uniform(10, self.image_size - 10),
                'vx': np.random.uniform(-4, 4),
                'vy': np.random.uniform(-4, 4),
                'radius': np.random.randint(3, 8),
                'color': np.random.rand(self.in_channels) if self.in_channels > 1 else np.array([1.0])
            }
            balls.append(ball)
        
        for t in range(self.seq_length):
            # Create frame
            frame = torch.zeros(self.in_channels, self.image_size, self.image_size)
            
            for ball in balls:
                # Update position
                ball['x'] += ball['vx']
                ball['y'] += ball['vy']
                
                # Bounce off walls
                r = ball['radius']
                if ball['x'] <= r or ball['x'] >= self.image_size - r:
                    ball['vx'] = -ball['vx']
                    ball['x'] = np.clip(ball['x'], r, self.image_size - r)
                if ball['y'] <= r or ball['y'] >= self.image_size - r:
                    ball['vy'] = -ball['vy']
                    ball['y'] = np.clip(ball['y'], r, self.image_size - r)
                
                # Draw ball (using vectorized operations for efficiency)
                y_coords, x_coords = torch.meshgrid(
                    torch.arange(self.image_size),
                    torch.arange(self.image_size),
                    indexing='ij'
                )
                dist_sq = (y_coords - ball['y']) ** 2 + (x_coords - ball['x']) ** 2
                mask = dist_sq <= ball['radius'] ** 2
                
                for c in range(self.in_channels):
                    frame[c][mask] = ball['color'][c]
            
            frames[t] = frame
        
        return frames


def get_dataloader(
    dataset: Dataset,
    batch_size: int,
    shuffle: bool = True,
    num_workers: int = 4,
    pin_memory: bool = True,
    drop_last: bool = True,
    persistent_workers: bool = False
) -> DataLoader:
    """
    Create a DataLoader with appropriate settings.
    
    Args:
        dataset: The dataset to load from.
        batch_size: Batch size.
        shuffle: Whether to shuffle data.
        num_workers: Number of worker processes.
        pin_memory: Whether to pin memory for faster GPU transfer.
        drop_last: Whether to drop the last incomplete batch.
        persistent_workers: Keep workers alive between epochs.
        
    Returns:
        DataLoader instance.
    """
    return DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        pin_memory=pin_memory,
        drop_last=drop_last,
        persistent_workers=persistent_workers if num_workers > 0 else False,
        collate_fn=collate_sequences
    )


def collate_sequences(batch: list) -> torch.Tensor:
    """
    Collate function for sequence batches.
    
    Handles variable length sequences by trimming to the minimum length.
    
    Args:
        batch: List of tensors, each of shape (T, C, H, W).
        
    Returns:
        Batched tensor of shape (B, T, C, H, W).
    """
    # Find minimum sequence length
    min_length = min(seq.shape[0] for seq in batch)
    
    # Trim all sequences to minimum length
    trimmed = [seq[:min_length] for seq in batch]
    
    # Stack into batch
    return torch.stack(trimmed, dim=0)  # (B, T, C, H, W)


def create_synthetic_dataset(
    n_train: int = 10000,
    n_val: int = 1000,
    seq_length: int = 20,
    image_size: int = 64,
    n_balls: int = 2,
    in_channels: int = 3
) -> Tuple[SyntheticPhysicsDataset, SyntheticPhysicsDataset]:
    """
    Create synthetic train and validation datasets.
    
    Args:
        n_train: Number of training samples.
        n_val: Number of validation samples.
        seq_length: Frames per sequence.
        image_size: Image size.
        n_balls: Balls per sequence.
        in_channels: Number of channels.
        
    Returns:
        Tuple of (train_dataset, val_dataset).
    """
    train_dataset = SyntheticPhysicsDataset(
        n_samples=n_train,
        seq_length=seq_length,
        image_size=image_size,
        n_balls=n_balls,
        in_channels=in_channels
    )
    
    val_dataset = SyntheticPhysicsDataset(
        n_samples=n_val,
        seq_length=seq_length,
        image_size=image_size,
        n_balls=n_balls,
        in_channels=in_channels
    )
    
    return train_dataset, val_dataset
